{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.linear_model import SGDClassifier as SGD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('task_b.csv')\n",
    "data=data.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-195.871045</td>\n",
       "      <td>-14843.084171</td>\n",
       "      <td>5.532140</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1217.183964</td>\n",
       "      <td>-4068.124621</td>\n",
       "      <td>4.416082</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.138451</td>\n",
       "      <td>4413.412028</td>\n",
       "      <td>0.425317</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>363.824242</td>\n",
       "      <td>15474.760647</td>\n",
       "      <td>1.094119</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-768.812047</td>\n",
       "      <td>-7963.932192</td>\n",
       "      <td>1.870536</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            f1            f2        f3    y\n",
       "0  -195.871045 -14843.084171  5.532140  1.0\n",
       "1 -1217.183964  -4068.124621  4.416082  1.0\n",
       "2     9.138451   4413.412028  0.425317  0.0\n",
       "3   363.824242  15474.760647  1.094119  0.0\n",
       "4  -768.812047  -7963.932192  1.870536  0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1    0.067172\n",
       "f2   -0.017944\n",
       "f3    0.839060\n",
       "y     1.000000\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr()['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f1      488.195035\n",
       "f2    10403.417325\n",
       "f3        2.926662\n",
       "y         0.501255\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 3)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "X=data[['f1','f2','f3']].values\n",
    "Y=data['y'].values\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Applying Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
       "              fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
       "              loss='log', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
       "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SGD(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 1.08, NNZs: 3, Bias: -0.001751, T: 200, Avg. loss: 2516.147588\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.61, NNZs: 3, Bias: -0.001551, T: 400, Avg. loss: 2621.694380\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.35, NNZs: 3, Bias: -0.001850, T: 600, Avg. loss: 3285.222158\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.64, NNZs: 3, Bias: -0.003527, T: 800, Avg. loss: 3142.216822\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.48, NNZs: 3, Bias: -0.004027, T: 1000, Avg. loss: 3009.886714\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.40, NNZs: 3, Bias: -0.003523, T: 1200, Avg. loss: 3032.001946\n",
      "Total training time: 0.01 seconds.\n",
      "Convergence after 6 epochs took 0.01 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
       "              fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
       "              loss='log', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
       "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=X,y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.37170471, -1.34463853,  0.12669033]]), array([-0.00352309]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_, clf.intercept_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "1. Second feature got the maximum value in the coefficient array.\n",
    "2. Negative value for feature(f2) indicates that it pushes the classification more towards negative class\n",
    "3. First feature has maximum positive value in coefficient array means that it maximum towards positive class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Applying SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGD(eta0=0.0001, alpha=0.0001, loss='hinge', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.61, NNZs: 3, Bias: -0.001600, T: 200, Avg. loss: 2634.084615\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.68, NNZs: 3, Bias: -0.001100, T: 400, Avg. loss: 2593.136418\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.76, NNZs: 3, Bias: -0.000900, T: 600, Avg. loss: 3308.216351\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.77, NNZs: 3, Bias: -0.002700, T: 800, Avg. loss: 3155.085896\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.93, NNZs: 3, Bias: -0.002800, T: 1000, Avg. loss: 3080.501847\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.43, NNZs: 3, Bias: -0.002700, T: 1200, Avg. loss: 3011.887174\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.69, NNZs: 3, Bias: -0.002200, T: 1400, Avg. loss: 3002.132514\n",
      "Total training time: 0.02 seconds.\n",
      "Convergence after 7 epochs took 0.02 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
       "              fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
       "              loss='hinge', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
       "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=X,y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.38249139, -0.55764501,  0.15407861]]), array([-0.0022]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_, clf.intercept_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "1. Second feature got the maximum value in the coefficient array.\n",
    "2. Negative value for feature(f2) indicates that it pushes the classification more towards negative class\n",
    "3. First feature has maximum positive value in coefficient array means that it maximize towards positive class  \n",
    "4. Although SVM also finds feature(f2) as most important feature towards classifying in negative class but effect is less than \n",
    "   was in logictic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-195.871045</td>\n",
       "      <td>-14843.084171</td>\n",
       "      <td>5.532140</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1217.183964</td>\n",
       "      <td>-4068.124621</td>\n",
       "      <td>4.416082</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.138451</td>\n",
       "      <td>4413.412028</td>\n",
       "      <td>0.425317</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>363.824242</td>\n",
       "      <td>15474.760647</td>\n",
       "      <td>1.094119</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-768.812047</td>\n",
       "      <td>-7963.932192</td>\n",
       "      <td>1.870536</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            f1            f2        f3    y\n",
       "0  -195.871045 -14843.084171  5.532140  1.0\n",
       "1 -1217.183964  -4068.124621  4.416082  1.0\n",
       "2     9.138451   4413.412028  0.425317  0.0\n",
       "3   363.824242  15474.760647  1.094119  0.0\n",
       "4  -768.812047  -7963.932192  1.870536  0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('task_b.csv')\n",
    "data=data.iloc[:,1:]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 3)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "X=data[['f1','f2','f3']].values\n",
    "Y=data['y'].values\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Standardizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = StandardScaler()\n",
    "Std_X = std.fit_transform(X,Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Applying LR on Standardized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
       "              fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
       "              loss='log', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
       "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SGD(eta0=0.0001, alpha=0.0001, loss='log', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.01, NNZs: 3, Bias: 0.000001, T: 200, Avg. loss: 0.691431\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.02, NNZs: 3, Bias: 0.000002, T: 400, Avg. loss: 0.687922\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.03, NNZs: 3, Bias: 0.000002, T: 600, Avg. loss: 0.684449\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.03, NNZs: 3, Bias: 0.000003, T: 800, Avg. loss: 0.681011\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.04, NNZs: 3, Bias: 0.000003, T: 1000, Avg. loss: 0.677608\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.05, NNZs: 3, Bias: 0.000003, T: 1200, Avg. loss: 0.674240\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.06, NNZs: 3, Bias: 0.000003, T: 1400, Avg. loss: 0.670905\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.07, NNZs: 3, Bias: 0.000003, T: 1600, Avg. loss: 0.667605\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.07, NNZs: 3, Bias: 0.000002, T: 1800, Avg. loss: 0.664338\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.08, NNZs: 3, Bias: 0.000002, T: 2000, Avg. loss: 0.661104\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.09, NNZs: 3, Bias: 0.000002, T: 2200, Avg. loss: 0.657903\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.10, NNZs: 3, Bias: 0.000001, T: 2400, Avg. loss: 0.654734\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.11, NNZs: 3, Bias: 0.000001, T: 2600, Avg. loss: 0.651598\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 0.11, NNZs: 3, Bias: 0.000001, T: 2800, Avg. loss: 0.648493\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 0.12, NNZs: 3, Bias: 0.000001, T: 3000, Avg. loss: 0.645419\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 0.13, NNZs: 3, Bias: 0.000002, T: 3200, Avg. loss: 0.642377\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 0.14, NNZs: 3, Bias: 0.000002, T: 3400, Avg. loss: 0.639365\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 0.14, NNZs: 3, Bias: 0.000001, T: 3600, Avg. loss: 0.636383\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 0.15, NNZs: 3, Bias: 0.000001, T: 3800, Avg. loss: 0.633431\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 0.16, NNZs: 3, Bias: 0.000001, T: 4000, Avg. loss: 0.630509\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 0.17, NNZs: 3, Bias: 0.000001, T: 4200, Avg. loss: 0.627616\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 0.18, NNZs: 3, Bias: 0.000001, T: 4400, Avg. loss: 0.624752\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 0.18, NNZs: 3, Bias: 0.000001, T: 4600, Avg. loss: 0.621917\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 0.19, NNZs: 3, Bias: 0.000000, T: 4800, Avg. loss: 0.619110\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 0.20, NNZs: 3, Bias: -0.000000, T: 5000, Avg. loss: 0.616331\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 0.21, NNZs: 3, Bias: -0.000001, T: 5200, Avg. loss: 0.613579\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 0.21, NNZs: 3, Bias: -0.000001, T: 5400, Avg. loss: 0.610855\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 0.22, NNZs: 3, Bias: -0.000001, T: 5600, Avg. loss: 0.608158\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 0.23, NNZs: 3, Bias: -0.000002, T: 5800, Avg. loss: 0.605488\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 0.23, NNZs: 3, Bias: -0.000003, T: 6000, Avg. loss: 0.602845\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 0.24, NNZs: 3, Bias: -0.000002, T: 6200, Avg. loss: 0.600227\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 0.25, NNZs: 3, Bias: -0.000002, T: 6400, Avg. loss: 0.597635\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 0.26, NNZs: 3, Bias: -0.000003, T: 6600, Avg. loss: 0.595069\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 0.26, NNZs: 3, Bias: -0.000003, T: 6800, Avg. loss: 0.592528\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 0.27, NNZs: 3, Bias: -0.000003, T: 7000, Avg. loss: 0.590011\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 0.28, NNZs: 3, Bias: -0.000004, T: 7200, Avg. loss: 0.587519\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 0.28, NNZs: 3, Bias: -0.000004, T: 7400, Avg. loss: 0.585052\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 0.29, NNZs: 3, Bias: -0.000004, T: 7600, Avg. loss: 0.582608\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 0.30, NNZs: 3, Bias: -0.000005, T: 7800, Avg. loss: 0.580189\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 0.30, NNZs: 3, Bias: -0.000005, T: 8000, Avg. loss: 0.577793\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 0.31, NNZs: 3, Bias: -0.000007, T: 8200, Avg. loss: 0.575420\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 0.32, NNZs: 3, Bias: -0.000008, T: 8400, Avg. loss: 0.573071\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 0.33, NNZs: 3, Bias: -0.000009, T: 8600, Avg. loss: 0.570743\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 0.33, NNZs: 3, Bias: -0.000010, T: 8800, Avg. loss: 0.568439\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 0.34, NNZs: 3, Bias: -0.000010, T: 9000, Avg. loss: 0.566156\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 0.35, NNZs: 3, Bias: -0.000010, T: 9200, Avg. loss: 0.563896\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 0.35, NNZs: 3, Bias: -0.000012, T: 9400, Avg. loss: 0.561657\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 0.36, NNZs: 3, Bias: -0.000012, T: 9600, Avg. loss: 0.559439\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 0.37, NNZs: 3, Bias: -0.000013, T: 9800, Avg. loss: 0.557243\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 0.37, NNZs: 3, Bias: -0.000015, T: 10000, Avg. loss: 0.555067\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 0.38, NNZs: 3, Bias: -0.000016, T: 10200, Avg. loss: 0.552912\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 0.39, NNZs: 3, Bias: -0.000017, T: 10400, Avg. loss: 0.550777\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 0.39, NNZs: 3, Bias: -0.000018, T: 10600, Avg. loss: 0.548663\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 0.40, NNZs: 3, Bias: -0.000020, T: 10800, Avg. loss: 0.546568\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 0.40, NNZs: 3, Bias: -0.000021, T: 11000, Avg. loss: 0.544493\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 0.41, NNZs: 3, Bias: -0.000023, T: 11200, Avg. loss: 0.542438\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 0.42, NNZs: 3, Bias: -0.000024, T: 11400, Avg. loss: 0.540402\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 0.42, NNZs: 3, Bias: -0.000025, T: 11600, Avg. loss: 0.538384\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 0.43, NNZs: 3, Bias: -0.000027, T: 11800, Avg. loss: 0.536386\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 0.44, NNZs: 3, Bias: -0.000028, T: 12000, Avg. loss: 0.534406\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 0.44, NNZs: 3, Bias: -0.000030, T: 12200, Avg. loss: 0.532444\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 0.45, NNZs: 3, Bias: -0.000030, T: 12400, Avg. loss: 0.530501\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 0.45, NNZs: 3, Bias: -0.000032, T: 12600, Avg. loss: 0.528575\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 0.46, NNZs: 3, Bias: -0.000034, T: 12800, Avg. loss: 0.526668\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 0.47, NNZs: 3, Bias: -0.000035, T: 13000, Avg. loss: 0.524777\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 0.47, NNZs: 3, Bias: -0.000038, T: 13200, Avg. loss: 0.522904\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 0.48, NNZs: 3, Bias: -0.000039, T: 13400, Avg. loss: 0.521048\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 0.49, NNZs: 3, Bias: -0.000042, T: 13600, Avg. loss: 0.519209\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 0.49, NNZs: 3, Bias: -0.000046, T: 13800, Avg. loss: 0.517387\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 0.50, NNZs: 3, Bias: -0.000049, T: 14000, Avg. loss: 0.515581\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 0.50, NNZs: 3, Bias: -0.000054, T: 14200, Avg. loss: 0.513791\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 0.51, NNZs: 3, Bias: -0.000057, T: 14400, Avg. loss: 0.512018\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 0.52, NNZs: 3, Bias: -0.000060, T: 14600, Avg. loss: 0.510260\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 0.52, NNZs: 3, Bias: -0.000064, T: 14800, Avg. loss: 0.508518\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 0.53, NNZs: 3, Bias: -0.000067, T: 15000, Avg. loss: 0.506792\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 0.53, NNZs: 3, Bias: -0.000069, T: 15200, Avg. loss: 0.505081\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 0.54, NNZs: 3, Bias: -0.000072, T: 15400, Avg. loss: 0.503385\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 0.54, NNZs: 3, Bias: -0.000076, T: 15600, Avg. loss: 0.501705\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 0.55, NNZs: 3, Bias: -0.000080, T: 15800, Avg. loss: 0.500039\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 0.56, NNZs: 3, Bias: -0.000084, T: 16000, Avg. loss: 0.498387\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 0.56, NNZs: 3, Bias: -0.000088, T: 16200, Avg. loss: 0.496751\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 0.57, NNZs: 3, Bias: -0.000091, T: 16400, Avg. loss: 0.495128\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 0.57, NNZs: 3, Bias: -0.000095, T: 16600, Avg. loss: 0.493520\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 0.58, NNZs: 3, Bias: -0.000100, T: 16800, Avg. loss: 0.491925\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 0.58, NNZs: 3, Bias: -0.000104, T: 17000, Avg. loss: 0.490345\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 0.59, NNZs: 3, Bias: -0.000108, T: 17200, Avg. loss: 0.488778\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 0.60, NNZs: 3, Bias: -0.000113, T: 17400, Avg. loss: 0.487225\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 0.60, NNZs: 3, Bias: -0.000118, T: 17600, Avg. loss: 0.485685\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 0.61, NNZs: 3, Bias: -0.000123, T: 17800, Avg. loss: 0.484158\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 0.61, NNZs: 3, Bias: -0.000128, T: 18000, Avg. loss: 0.482644\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 0.62, NNZs: 3, Bias: -0.000132, T: 18200, Avg. loss: 0.481144\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 0.62, NNZs: 3, Bias: -0.000137, T: 18400, Avg. loss: 0.479656\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 0.63, NNZs: 3, Bias: -0.000143, T: 18600, Avg. loss: 0.478180\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 0.63, NNZs: 3, Bias: -0.000147, T: 18800, Avg. loss: 0.476718\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 0.64, NNZs: 3, Bias: -0.000153, T: 19000, Avg. loss: 0.475267\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 0.64, NNZs: 3, Bias: -0.000158, T: 19200, Avg. loss: 0.473829\n",
      "Total training time: 0.11 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 0.65, NNZs: 3, Bias: -0.000164, T: 19400, Avg. loss: 0.472402\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 0.65, NNZs: 3, Bias: -0.000170, T: 19600, Avg. loss: 0.470988\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 0.66, NNZs: 3, Bias: -0.000176, T: 19800, Avg. loss: 0.469585\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 0.67, NNZs: 3, Bias: -0.000182, T: 20000, Avg. loss: 0.468194\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 0.67, NNZs: 3, Bias: -0.000188, T: 20200, Avg. loss: 0.466815\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 0.68, NNZs: 3, Bias: -0.000194, T: 20400, Avg. loss: 0.465447\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 0.68, NNZs: 3, Bias: -0.000201, T: 20600, Avg. loss: 0.464090\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 0.69, NNZs: 3, Bias: -0.000207, T: 20800, Avg. loss: 0.462744\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 0.69, NNZs: 3, Bias: -0.000215, T: 21000, Avg. loss: 0.461410\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 0.70, NNZs: 3, Bias: -0.000222, T: 21200, Avg. loss: 0.460086\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 0.70, NNZs: 3, Bias: -0.000228, T: 21400, Avg. loss: 0.458773\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 0.71, NNZs: 3, Bias: -0.000235, T: 21600, Avg. loss: 0.457471\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 0.71, NNZs: 3, Bias: -0.000241, T: 21800, Avg. loss: 0.456179\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 0.72, NNZs: 3, Bias: -0.000249, T: 22000, Avg. loss: 0.454898\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 0.72, NNZs: 3, Bias: -0.000255, T: 22200, Avg. loss: 0.453627\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 0.73, NNZs: 3, Bias: -0.000262, T: 22400, Avg. loss: 0.452366\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 0.73, NNZs: 3, Bias: -0.000269, T: 22600, Avg. loss: 0.451115\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 0.74, NNZs: 3, Bias: -0.000277, T: 22800, Avg. loss: 0.449874\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 0.74, NNZs: 3, Bias: -0.000284, T: 23000, Avg. loss: 0.448643\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 0.75, NNZs: 3, Bias: -0.000292, T: 23200, Avg. loss: 0.447422\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 0.75, NNZs: 3, Bias: -0.000300, T: 23400, Avg. loss: 0.446210\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 0.76, NNZs: 3, Bias: -0.000308, T: 23600, Avg. loss: 0.445008\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 0.76, NNZs: 3, Bias: -0.000316, T: 23800, Avg. loss: 0.443816\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 0.77, NNZs: 3, Bias: -0.000326, T: 24000, Avg. loss: 0.442632\n",
      "Total training time: 0.13 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 0.77, NNZs: 3, Bias: -0.000334, T: 24200, Avg. loss: 0.441458\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 0.78, NNZs: 3, Bias: -0.000343, T: 24400, Avg. loss: 0.440293\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 0.78, NNZs: 3, Bias: -0.000352, T: 24600, Avg. loss: 0.439138\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 0.79, NNZs: 3, Bias: -0.000362, T: 24800, Avg. loss: 0.437991\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 0.79, NNZs: 3, Bias: -0.000370, T: 25000, Avg. loss: 0.436852\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 0.79, NNZs: 3, Bias: -0.000378, T: 25200, Avg. loss: 0.435723\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 0.80, NNZs: 3, Bias: -0.000388, T: 25400, Avg. loss: 0.434602\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 0.80, NNZs: 3, Bias: -0.000397, T: 25600, Avg. loss: 0.433490\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 0.81, NNZs: 3, Bias: -0.000407, T: 25800, Avg. loss: 0.432387\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 0.81, NNZs: 3, Bias: -0.000417, T: 26000, Avg. loss: 0.431291\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 0.82, NNZs: 3, Bias: -0.000426, T: 26200, Avg. loss: 0.430204\n",
      "Total training time: 0.14 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 0.82, NNZs: 3, Bias: -0.000436, T: 26400, Avg. loss: 0.429125\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 0.83, NNZs: 3, Bias: -0.000446, T: 26600, Avg. loss: 0.428055\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 0.83, NNZs: 3, Bias: -0.000457, T: 26800, Avg. loss: 0.426992\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 0.84, NNZs: 3, Bias: -0.000467, T: 27000, Avg. loss: 0.425937\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 0.84, NNZs: 3, Bias: -0.000478, T: 27200, Avg. loss: 0.424891\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 0.85, NNZs: 3, Bias: -0.000490, T: 27400, Avg. loss: 0.423851\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 0.85, NNZs: 3, Bias: -0.000501, T: 27600, Avg. loss: 0.422820\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 0.85, NNZs: 3, Bias: -0.000512, T: 27800, Avg. loss: 0.421796\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 0.86, NNZs: 3, Bias: -0.000523, T: 28000, Avg. loss: 0.420780\n",
      "Total training time: 0.15 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 0.86, NNZs: 3, Bias: -0.000534, T: 28200, Avg. loss: 0.419771\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 0.87, NNZs: 3, Bias: -0.000546, T: 28400, Avg. loss: 0.418770\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 0.87, NNZs: 3, Bias: -0.000556, T: 28600, Avg. loss: 0.417776\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 0.88, NNZs: 3, Bias: -0.000568, T: 28800, Avg. loss: 0.416789\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 0.88, NNZs: 3, Bias: -0.000579, T: 29000, Avg. loss: 0.415809\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 0.89, NNZs: 3, Bias: -0.000591, T: 29200, Avg. loss: 0.414836\n",
      "Total training time: 0.16 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 0.89, NNZs: 3, Bias: -0.000603, T: 29400, Avg. loss: 0.413871\n",
      "Total training time: 0.16 seconds.\n",
      "Convergence after 147 epochs took 0.16 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
       "              fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
       "              loss='log', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
       "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=Std_X,y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.03851242, -0.00553122,  0.88963322]]), array([-0.00060297]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_, clf.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "1. After applying standard scaling on the data we can observe huge change in loss which comes down from 3032 to 0.413\n",
    "2. There is also change in the values of coefficients.\n",
    "3. 3rd feature(f3) becomes the most important feature as it has maximum value.\n",
    "4. 2nd feature(f4) still has negative value implies that it most important feature in classifying the point as negative "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Applying SVM on Standardized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGD(eta0=0.0001, alpha=0.0001, loss='hinge', random_state=15, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.02, NNZs: 3, Bias: 0.000000, T: 200, Avg. loss: 0.993111\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.03, NNZs: 3, Bias: -0.000000, T: 400, Avg. loss: 0.978934\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.05, NNZs: 3, Bias: 0.000000, T: 600, Avg. loss: 0.964757\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.07, NNZs: 3, Bias: 0.000000, T: 800, Avg. loss: 0.950580\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.08, NNZs: 3, Bias: -0.000000, T: 1000, Avg. loss: 0.936403\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 0.10, NNZs: 3, Bias: 0.000000, T: 1200, Avg. loss: 0.922226\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 0.12, NNZs: 3, Bias: 0.000000, T: 1400, Avg. loss: 0.908049\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 0.13, NNZs: 3, Bias: 0.000000, T: 1600, Avg. loss: 0.893873\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 0.15, NNZs: 3, Bias: 0.000000, T: 1800, Avg. loss: 0.879696\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 0.17, NNZs: 3, Bias: 0.000000, T: 2000, Avg. loss: 0.865519\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 0.19, NNZs: 3, Bias: -0.000000, T: 2200, Avg. loss: 0.851342\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 0.20, NNZs: 3, Bias: -0.000000, T: 2400, Avg. loss: 0.837165\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 0.22, NNZs: 3, Bias: -0.000000, T: 2600, Avg. loss: 0.822988\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 0.24, NNZs: 3, Bias: -0.000000, T: 2800, Avg. loss: 0.808812\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 0.25, NNZs: 3, Bias: 0.000000, T: 3000, Avg. loss: 0.794635\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 0.27, NNZs: 3, Bias: 0.000000, T: 3200, Avg. loss: 0.780458\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 0.29, NNZs: 3, Bias: -0.000000, T: 3400, Avg. loss: 0.766282\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 0.30, NNZs: 3, Bias: 0.000000, T: 3600, Avg. loss: 0.752105\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 0.32, NNZs: 3, Bias: 0.000000, T: 3800, Avg. loss: 0.737929\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 0.34, NNZs: 3, Bias: -0.000000, T: 4000, Avg. loss: 0.723752\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 0.35, NNZs: 3, Bias: 0.000000, T: 4200, Avg. loss: 0.709575\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 0.37, NNZs: 3, Bias: 0.000000, T: 4400, Avg. loss: 0.695399\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 0.39, NNZs: 3, Bias: -0.000000, T: 4600, Avg. loss: 0.681222\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 0.40, NNZs: 3, Bias: 0.000000, T: 4800, Avg. loss: 0.667046\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 0.42, NNZs: 3, Bias: 0.000000, T: 5000, Avg. loss: 0.652870\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 0.44, NNZs: 3, Bias: 0.000000, T: 5200, Avg. loss: 0.638693\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 0.45, NNZs: 3, Bias: 0.000000, T: 5400, Avg. loss: 0.624517\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 0.47, NNZs: 3, Bias: -0.000000, T: 5600, Avg. loss: 0.610341\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 0.49, NNZs: 3, Bias: -0.000000, T: 5800, Avg. loss: 0.596164\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 0.51, NNZs: 3, Bias: -0.000000, T: 6000, Avg. loss: 0.581988\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 0.52, NNZs: 3, Bias: -0.000000, T: 6200, Avg. loss: 0.567812\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 0.54, NNZs: 3, Bias: -0.000000, T: 6400, Avg. loss: 0.553635\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 0.56, NNZs: 3, Bias: -0.000100, T: 6600, Avg. loss: 0.539559\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 0.57, NNZs: 3, Bias: -0.000400, T: 6800, Avg. loss: 0.525831\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 0.59, NNZs: 3, Bias: -0.000400, T: 7000, Avg. loss: 0.512822\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 0.60, NNZs: 3, Bias: -0.000300, T: 7200, Avg. loss: 0.500814\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 0.62, NNZs: 3, Bias: 0.000100, T: 7400, Avg. loss: 0.489867\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 0.63, NNZs: 3, Bias: 0.000500, T: 7600, Avg. loss: 0.479843\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 0.64, NNZs: 3, Bias: 0.000900, T: 7800, Avg. loss: 0.470023\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 0.66, NNZs: 3, Bias: 0.001200, T: 8000, Avg. loss: 0.460939\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 0.67, NNZs: 3, Bias: 0.001100, T: 8200, Avg. loss: 0.453123\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 0.68, NNZs: 3, Bias: 0.001000, T: 8400, Avg. loss: 0.446506\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 0.69, NNZs: 3, Bias: 0.001000, T: 8600, Avg. loss: 0.440244\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 0.70, NNZs: 3, Bias: 0.001000, T: 8800, Avg. loss: 0.434183\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 0.71, NNZs: 3, Bias: 0.001200, T: 9000, Avg. loss: 0.428457\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 0.72, NNZs: 3, Bias: 0.001400, T: 9200, Avg. loss: 0.422915\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 0.73, NNZs: 3, Bias: 0.001500, T: 9400, Avg. loss: 0.417423\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 0.74, NNZs: 3, Bias: 0.001800, T: 9600, Avg. loss: 0.412336\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 0.75, NNZs: 3, Bias: 0.001900, T: 9800, Avg. loss: 0.407610\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 0.76, NNZs: 3, Bias: 0.002200, T: 10000, Avg. loss: 0.403083\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 0.77, NNZs: 3, Bias: 0.002500, T: 10200, Avg. loss: 0.398796\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 0.78, NNZs: 3, Bias: 0.002900, T: 10400, Avg. loss: 0.394692\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 0.79, NNZs: 3, Bias: 0.003400, T: 10600, Avg. loss: 0.390702\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 0.80, NNZs: 3, Bias: 0.004000, T: 10800, Avg. loss: 0.386766\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 0.81, NNZs: 3, Bias: 0.004800, T: 11000, Avg. loss: 0.383045\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 0.81, NNZs: 3, Bias: 0.005600, T: 11200, Avg. loss: 0.379523\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 0.82, NNZs: 3, Bias: 0.006400, T: 11400, Avg. loss: 0.376092\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 0.83, NNZs: 3, Bias: 0.006900, T: 11600, Avg. loss: 0.372853\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 0.84, NNZs: 3, Bias: 0.007400, T: 11800, Avg. loss: 0.369757\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 0.85, NNZs: 3, Bias: 0.007900, T: 12000, Avg. loss: 0.366660\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 0.85, NNZs: 3, Bias: 0.008500, T: 12200, Avg. loss: 0.363659\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 0.86, NNZs: 3, Bias: 0.008900, T: 12400, Avg. loss: 0.360918\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 0.87, NNZs: 3, Bias: 0.009300, T: 12600, Avg. loss: 0.358293\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 0.87, NNZs: 3, Bias: 0.009700, T: 12800, Avg. loss: 0.355741\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 0.88, NNZs: 3, Bias: 0.010200, T: 13000, Avg. loss: 0.353291\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 0.89, NNZs: 3, Bias: 0.010500, T: 13200, Avg. loss: 0.350947\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 0.89, NNZs: 3, Bias: 0.010800, T: 13400, Avg. loss: 0.348714\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 0.90, NNZs: 3, Bias: 0.011000, T: 13600, Avg. loss: 0.346524\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 0.91, NNZs: 3, Bias: 0.011000, T: 13800, Avg. loss: 0.344410\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 0.91, NNZs: 3, Bias: 0.011000, T: 14000, Avg. loss: 0.342392\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 0.92, NNZs: 3, Bias: 0.010900, T: 14200, Avg. loss: 0.340372\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 0.93, NNZs: 3, Bias: 0.010800, T: 14400, Avg. loss: 0.338400\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 0.93, NNZs: 3, Bias: 0.010600, T: 14600, Avg. loss: 0.336501\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 0.94, NNZs: 3, Bias: 0.010400, T: 14800, Avg. loss: 0.334604\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 0.94, NNZs: 3, Bias: 0.010400, T: 15000, Avg. loss: 0.332782\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 0.95, NNZs: 3, Bias: 0.010300, T: 15200, Avg. loss: 0.331037\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 0.96, NNZs: 3, Bias: 0.010200, T: 15400, Avg. loss: 0.329317\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 0.96, NNZs: 3, Bias: 0.010100, T: 15600, Avg. loss: 0.327597\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 0.97, NNZs: 3, Bias: 0.009900, T: 15800, Avg. loss: 0.325932\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 0.97, NNZs: 3, Bias: 0.009900, T: 16000, Avg. loss: 0.324369\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 0.98, NNZs: 3, Bias: 0.009900, T: 16200, Avg. loss: 0.322840\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 0.98, NNZs: 3, Bias: 0.009900, T: 16400, Avg. loss: 0.321310\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 0.99, NNZs: 3, Bias: 0.010000, T: 16600, Avg. loss: 0.319872\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 0.99, NNZs: 3, Bias: 0.010100, T: 16800, Avg. loss: 0.318513\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 1.00, NNZs: 3, Bias: 0.010300, T: 17000, Avg. loss: 0.317175\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 1.00, NNZs: 3, Bias: 0.010500, T: 17200, Avg. loss: 0.315862\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 1.01, NNZs: 3, Bias: 0.010700, T: 17400, Avg. loss: 0.314549\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 1.01, NNZs: 3, Bias: 0.010900, T: 17600, Avg. loss: 0.313237\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 1.02, NNZs: 3, Bias: 0.011100, T: 17800, Avg. loss: 0.311924\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 1.02, NNZs: 3, Bias: 0.011300, T: 18000, Avg. loss: 0.310612\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 1.03, NNZs: 3, Bias: 0.011500, T: 18200, Avg. loss: 0.309337\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 1.03, NNZs: 3, Bias: 0.011700, T: 18400, Avg. loss: 0.308166\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 1.04, NNZs: 3, Bias: 0.011900, T: 18600, Avg. loss: 0.307057\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 1.04, NNZs: 3, Bias: 0.012200, T: 18800, Avg. loss: 0.305980\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 1.05, NNZs: 3, Bias: 0.012500, T: 19000, Avg. loss: 0.304914\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 1.05, NNZs: 3, Bias: 0.012800, T: 19200, Avg. loss: 0.303880\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 1.06, NNZs: 3, Bias: 0.013100, T: 19400, Avg. loss: 0.302892\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 1.06, NNZs: 3, Bias: 0.013500, T: 19600, Avg. loss: 0.301927\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 1.07, NNZs: 3, Bias: 0.013900, T: 19800, Avg. loss: 0.300979\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 1.07, NNZs: 3, Bias: 0.014100, T: 20000, Avg. loss: 0.300054\n",
      "Total training time: 0.08 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 1.07, NNZs: 3, Bias: 0.014300, T: 20200, Avg. loss: 0.299188\n",
      "Total training time: 0.08 seconds.\n",
      "Convergence after 101 epochs took 0.08 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0001,\n",
       "              fit_intercept=True, l1_ratio=0.15, learning_rate='constant',\n",
       "              loss='hinge', max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "              penalty='l2', power_t=0.5, random_state=15, shuffle=True,\n",
       "              tol=0.001, validation_fraction=0.1, verbose=2, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=Std_X,y=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.04248904, 0.02585179, 1.07272982]]), array([0.0143]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_, clf.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "1. After applying standard scaling on the data we can observe huge change in loss which comes down from 3002 to 0.299\n",
    "2. There is also change in the values of coefficients.\n",
    "3. 3rd feature(f3) becomes the most important feature as it has maximum value.\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
